package de.uni_mannheim.informatik.dws.melt.matching_ml.python.nlptransformers;

import de.uni_mannheim.informatik.dws.melt.matching_base.FileUtil;
import de.uni_mannheim.informatik.dws.melt.matching_base.Filter;
import java.io.File;
import java.util.List;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import de.uni_mannheim.informatik.dws.melt.matching_jena.TextExtractor;
import de.uni_mannheim.informatik.dws.melt.matching_jena.TextExtractorMap;
import de.uni_mannheim.informatik.dws.melt.matching_ml.python.PythonServer;
import java.util.Arrays;
import java.util.HashSet;
import java.util.Locale;
import java.util.Set;
import org.apache.commons.text.WordUtils;

/**
 * This filter asks a LLM which entity of the source fits best to an entity of the target.
 * Correspondences needs to be provided such that candidates are available.
 * It will only keep correspondences which are stated to be useful.
 * The difference to {@code #LLMBinaryFilter} is that all possible matches arte given to the LLM model.
 */
public abstract class LLMBase extends TransformersBase implements Filter {
    
    /**
     * The promt to use for the LLM.
     * Subclasses may interpret the promt differently.
     */
    protected String promt;
    
    /**
     * If set to a existing file, this class writes additional debug information to the corresponding file.
     */
    protected File debugFile;
    
    /**
     * If set to true, the generation will be stopped if yes or no words appear.
     */
    protected boolean wordStopper;
    
    /**
     * If set to true, the generation will be stopped if yes or no words appear.
     */
    protected boolean wordForcer;
    
    /**
     * Can add any parameter which are passed to the <a href="https://huggingface.co/transformers/main_classes/model.html#transformers.PreTrainedModel.from_pretrained">from_pretrained</a> method.
     */
    protected TransformersArguments loadingArguments;
    
    /**
     * Constructor with all required parameters and default values for optional parameters (can be changed by setters).
     * It uses the systems default tmp dir to store the files with texts generated from the knowledge graphs.
     * Pytorch is used instead of tensorflow and all visible GPUs are used for prediction.
     * @param extractor the extractor to select which text for each resource should be used.
     * @param modelName the model name which can be a model id (a hosted model on huggingface.co) or a path to a directory containing a model and tokenizer
     * (<a href="https://huggingface.co/transformers/main_classes/model.html#transformers.PreTrainedModel.from_pretrained">
     * see first parameter pretrained_model_name_or_path of the from_pretrained
     * function in huggingface library</a>). In case of a path, it should be absolute. 
     * The path can be generated by e.g. {@link FileUtil#getCanonicalPathIfPossible(java.io.File) }
     * @param promt The promt to use for the LLM. Use {left} and {right} to insert the text representation of the left and right concept.
     */
    public LLMBase(TextExtractorMap extractor, String modelName, String promt) {
        super(extractor, modelName);
        this.promt = promt;
        this.debugFile = null;
        this.wordStopper = true;
        this.wordForcer = false;
        this.loadingArguments = new TransformersArguments();
    }
    
    /**
     * Constructor with all required parameters and default values for optional parameters (can be changed by setters).
     * It uses the systems default tmp dir to store the files with texts generated from the knowledge graphs.
     * Pytorch is used instead of tensorflow and all visible GPUs are used for prediction.
     * @param extractor the extractor to select which text for each resource should be used.
     * @param modelName the model name which can be a model id (a hosted model on huggingface.co) or a path to a directory containing a model and tokenizer
     * (<a href="https://huggingface.co/transformers/main_classes/model.html#transformers.PreTrainedModel.from_pretrained">
     * see first parameter pretrained_model_name_or_path of the from_pretrained
     * function in huggingface library</a>). In case of a path, it should be absolute. 
     * The path can be generated by e.g. {@link FileUtil#getCanonicalPathIfPossible(java.io.File) }
     * @param promt The promt to use for the LLM. Use {left} and {right} to insert the text representation of the left and right concept.
     */
    public LLMBase(TextExtractor extractor, String modelName, String promt) {
        this(TextExtractorMap.wrapTextExtractor(extractor), modelName, promt);
    }
        
    /**
     * Run huggingface transformers library.
     * @param predictionFilePath path to csv file with two columns (text left and text right).
     * @param wordsToDetect the words which should be detected
     * @throws Exception in case something goes wrong.
     * @return a list of confidences
     */
    protected List<List<Double>> predictConfidences(File predictionFilePath, List<Set<String>> wordsToDetect) throws Exception{
        return PythonServer.getInstance().textGenerationPrediction(this, predictionFilePath, wordsToDetect);
    }

    public String getPromt() {
        return promt;
    }

    public void setPromt(String promt) {
        this.promt = promt;
    }

    public File getDebugFile() {
        return debugFile;
    }

    public void setDebugFile(File debugFile) {
        this.debugFile = debugFile;
    }

    public boolean isWordStopper() {
        return wordStopper;
    }

    /**
     * If set to true the text generation will automatically stop if the word yes or no is generated.
     * @param wordStopper fi true the generation stops on yes or no automatically.
     */
    public void setWordStopper(boolean wordStopper) {
        this.wordStopper = wordStopper;
    }

    public boolean isWordForcer() {
        return wordForcer;
    }

    /**
     * When setting this option to true, the <a href="https://huggingface.co/blog/constrained-beam-search">constrained beam search</a> is activated and the words yes and no will be forced.
     * This also means that the "num_beams" attribute in the generation arguments needs to be set to a number higher than one.
     * @param wordForcer true or false
     */
    public void setWordForcer(boolean wordForcer) {
        this.wordForcer = wordForcer;
    }
    
    /**
     * This functions add more word variations to the set of words.
     * This will be applied to all words in the set. Thus it should only contain similar words or variations.
     * This includes lower, upper, and title case as well as prefixing with space.
     * @param words words
     * @return all variation of the words.
     */
    public static Set<String> includeMoreVariations(String... words){
        return includeMoreVariations(new HashSet<>(Arrays.asList(words)));
    }
    
    /**
     * This functions add more word variations to the set of words.
     * This will be applied to all words in the set. Thus it should only contain similar words or variations.
     * This includes lower, upper, and title case as well as prefixing with space.
     * @param words words
     * @return all variation of the words.
     */
    public static Set<String> includeMoreVariations(Set<String> words){        
        Set<String> intermediate = new HashSet<>();
        for(String word : words){
            intermediate.add(word);
            intermediate.add("*" + word);
            intermediate.add(" " +word);
        }
        
        Set<String> finalSet = new HashSet<>();
        for(String word : intermediate){
            finalSet.add(word);
            finalSet.add(word.toLowerCase(Locale.ENGLISH));
            finalSet.add(word.toUpperCase(Locale.ENGLISH));
            finalSet.add(WordUtils.capitalizeFully(word, ' ', '*'));
        }
        
        return finalSet;
    }
    
    
    
    /**
     * Returns the arguments which can be used for the <a href="https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig">
     * generate function of transformers library</a>.
     * @return the generation arguments
     */
    public TransformersArguments getGenerationArguments() {
        return this.trainingArguments;
    }
    
    /**
     * Set the arguments which can be used for the <a href="https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig">
     * generate function of transformers library</a>.
     * @param generationArguments the new geenration arguments
     */
    public void setGenerationArguments(TransformersArguments generationArguments) {
        if(generationArguments == null)
            throw new IllegalArgumentException("generationArguments should not be set to null");
        this.trainingArguments = generationArguments;
    }
    
    /**
     * Add parameters which are passed to the <a href="https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig">
     * generate function of transformers library</a>.
     * @param key the key to use: <a href="https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig">possible options</a>.
     * @param value the corresponding value
     * @return the object to allow for further addGenerationArgument calls.
     */
    public LLMBase addGenerationArgument(String key, Object value) {
        if(this.trainingArguments == null)
            this.trainingArguments = new TransformersArguments();
        this.trainingArguments.addParameter(key, value);
        return this;
    }
    
    /**
     * Returns parameters which are passed to the <a href="https://huggingface.co/transformers/main_classes/model.html#transformers.PreTrainedModel.from_pretrained">from_pretrained</a> method.
     * @return the loading arguments.
     */
    public TransformersArguments getLoadingArguments() {
        return loadingArguments;
    }

    /**
     * Set the arguments which are passed to the <a href="https://huggingface.co/transformers/main_classes/model.html#transformers.PreTrainedModel.from_pretrained">from_pretrained</a> method.
     * @param loadingArguments new loading arguments
     */
    public void setLoadingArguments(TransformersArguments loadingArguments) {
        if(loadingArguments == null)
            throw new IllegalArgumentException("loadingArguments should not be set to null");
        this.loadingArguments = loadingArguments;
    }
    
    /**
     * Can add any parameter which are passed to the <a href="https://huggingface.co/transformers/main_classes/model.html#transformers.PreTrainedModel.from_pretrained">from_pretrained</a> method.
     * @param key the key to use e.g. load_in_8bit
     * @param value the corresponding value
     * @return the object to allow for further addGenerationArgument calls.
     */
    public LLMBase addLoadingArgument(String key, Object value) {
        if(this.loadingArguments == null)
            this.loadingArguments = new TransformersArguments();
        this.loadingArguments.addParameter(key, value);
        return this;
    }
    
    public LLMBase addLoadingArguments(TransformersArguments loadingArguments) {
        if(this.loadingArguments == null)
            this.loadingArguments = new TransformersArguments();
        this.loadingArguments.addAll(loadingArguments);
        return this;
    }
    
    /***********
     * Do not allow to set training arguments - not used for llms.
     **********************/
    
    @Override
    public void setTrainingArguments(TransformersArguments configuration) {
        throw new IllegalStateException("training arguments not used for LLMs");
    }
    
    @Override
    public void addTrainingArgument(String key, Object value){
        throw new IllegalStateException("training arguments not used for LLMs");
    }
    
    
    /**links:
     * https://huggingface.co/docs/transformers/generation_strategies
     * 
     * confidence:
     * https://discuss.huggingface.co/t/how-to-get-probability-of-next-word-from-an-lmmodel/26403
     * 
     * https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/text_generation#transformers.GenerationMixin.generate
     * 
        https://discuss.huggingface.co/t/announcement-generation-get-probabilities-for-generated-output/30075
        https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.compute_transition_scores
        https://huggingface.co/spaces/joaogante/color-coded-text-generation
        * 
        * 
        * 
        * pipeline text generation:
        * https://github.com/huggingface/transformers/blob/v4.31.0/src/transformers/pipelines/text_generation.py
     */
}
